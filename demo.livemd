# LSTM Demo

```elixir
# Mix.install([
#   {:axon, "~> 0.5.1"},
#   {:csv, "~> 3.0"},
#   {:tokenizers, "~> 0.3.0"},
#   {:table_rex, "~> 3.1.1"}
# ])
```

## Introduction

In Machine learning the process is always the same regardless of what technique is used:

1. Read the file and clean the data. Removing useless words like articles, special characters etc. Ensuring the most homogenous configuration as possible. This is important to feed our net to get the best learning.
2. Vectorized the information because ANNs only understand float instead of text
3. Build the model
4. Train the model
5. Run on test data
6. Finally test it with real data making predictions

<!-- livebook:{"break_markdown":true} -->

About Neural networks, RNNs and LSTMs:

![image](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)
![image](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

## Problem definition

* we are going to classify a complaints csv file generated by a call center
* data contains complaints in form of text and label specifying the type
* we want a way to easily figure out future complaint types, without human need

```elixir
Nx.default_backend({EXLA.Backend, client: :cuda})
```

## Read and clean the data

```elixir
symbols_to_be_replaced_by_space_regex = Regex.compile!("[/(){}\[\]\|@,;]")
symbols_to_be_removed_regex = Regex.compile!("[^a-z +_]")
censor_tokens_to_be_removed_regex = Regex.compile!("[^\s][@X/|\d{}]+[$\s.,\n]")
remaining_censor_tokens_to_be_removed_regex = Regex.compile!("[X]{2,}")

stopwords =
  "/data/english.stopwords"
  |> File.read!()
  |> String.split("\n")
  |> Enum.reject(&(&1 == ""))

clean_text = fn text ->
  text
  |> String.replace(censor_tokens_to_be_removed_regex, "")
  |> String.replace(remaining_censor_tokens_to_be_removed_regex, "")
  |> String.downcase()
  |> String.replace(symbols_to_be_replaced_by_space_regex, " ")
  |> String.replace(symbols_to_be_removed_regex, "")
  |> String.split()
  |> Enum.reject(&(&1 in stopwords))
  |> Enum.join(" ")
end
```

```elixir
data =
  "/data/data.csv"
  |> File.stream!()
  |> CSV.decode!(separator: ?#)
  |> Enum.drop(1)
  |> Enum.map(fn [_date, label, input_text] ->
    %{
      label: label,
      input_text: clean_text.(input_text)
    }
  end)
  |> Enum.to_list()
  |> Enum.shuffle()
```

```elixir
length = length(data)
split_index = round(length * 0.8)

train_data = Enum.slice(data, 0, split_index)
test_data = Enum.slice(data, split_index, length)

train_labels = Enum.map(train_data, & &1.label)
train_input_texts = Enum.map(train_data, & &1.input_text)

test_labels = Enum.map(test_data, & &1.label)
test_input_texts = Enum.map(test_data, & &1.input_text)
```

## Vectorize the information

```elixir
defmodule Vectorizer do
  alias Tokenizers.Tokenizer
  alias Tokenizers.Encoding

  defstruct [:tokenizer, :label_to_id, :id_to_label]

  @sequence_length 100

  def init do
    {:ok, tokenizer} = Tokenizer.from_pretrained("bert-base-cased")
    %Vectorizer{tokenizer: tokenizer}
  end

  def sequence_length, do: @sequence_length

  def encode_texts_to_tensor(%Vectorizer{tokenizer: tokenizer}, texts) do
    texts
    |> Enum.map(fn text ->
      {:ok, tokenized} = Tokenizer.encode(tokenizer, text)

      tokenized
      |> Encoding.pad(@sequence_length)
      |> Encoding.truncate(@sequence_length)
      |> Encoding.get_ids()
    end)
    |> Nx.tensor()
  end

  def fit_to_labels(%Vectorizer{} = vectorizer, labels) do
    labels_with_indices =
      labels
      |> Enum.uniq()
      |> Enum.with_index()

    %{
      vectorizer
      | label_to_id: Enum.into(labels_with_indices, %{}),
        id_to_label:
          labels_with_indices
          |> Enum.map(fn {label, id} -> {id, label} end)
          |> Enum.into(%{})
    }
  end

  def encode_labels_to_tensor(%Vectorizer{label_to_id: label_to_id} = vectorizer, labels) do
    labels
    |> Enum.map(&[Map.fetch!(label_to_id, &1)])
    |> Nx.tensor()
    |> Nx.equal(Nx.iota({1, length(unique_labels(vectorizer))}))
  end

  def unique_labels(%Vectorizer{label_to_id: label_to_id}) do
    Map.keys(label_to_id)
  end
end

vectorizer = Vectorizer.init() |> Vectorizer.fit_to_labels(train_labels)
```

```elixir
input_x =
  Vectorizer.encode_texts_to_tensor(vectorizer, train_input_texts)
  |> Nx.reshape({:auto, Vectorizer.sequence_length()})

unique_labels_count = Vectorizer.unique_labels(vectorizer) |> length()

input_y =
  vectorizer
  |> Vectorizer.encode_labels_to_tensor(train_labels)

{input_x, input_y}
```

## Build model

```elixir
model =
  Axon.input("complaints", shape: {nil, Vectorizer.sequence_length()})
  |> Axon.embedding(110, 120)
  |> Axon.lstm(50, activation: :tanh, gate: :hard_sigmoid)
  |> then(fn {output_sequence, {_new_cell, _new_hidden}} -> output_sequence end)
  |> Axon.dropout(rate: 0.25)
  |> Axon.lstm(20, activation: :tanh, gate: :hard_sigmoid)
  |> then(fn {output_sequence, {_new_cell, _new_hidden}} -> output_sequence end)
  |> Axon.nx(fn x -> x[[0..-1//1, -1]] end)
  |> Axon.dropout(rate: 0.45)
  |> Axon.dense(unique_labels_count, activation: :softmax)

model
|> Axon.Display.as_table(input_x)
|> IO.puts()
```

## Train the model

```elixir
batch_size = 32
batched_input_x = Nx.to_batched(input_x, batch_size)
batched_input_y = Nx.to_batched(input_y, batch_size)

model
|> Axon.Loop.trainer(:categorical_cross_entropy, :adam)
|> Axon.Loop.metric(:accuracy, "Accuracy")
|> Axon.Loop.run(
  Stream.zip(batched_input_x, batched_input_y),
  %{},
  epochs: 10,
  compiler: EXLA
)
```

<!-- livebook:{"branch_parent_index":3} -->

## Validate model

## Inference

```elixir
{
  Vectorizer.decode_texts_from_tensor(vectorizer, input_x),
  Vectorizer.decode_labels_from_tensor(vectorizer, input_y)
}
```
